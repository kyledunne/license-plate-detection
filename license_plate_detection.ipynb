{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import math\n",
    "import shutil\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary as torch_summary\n",
    "import cv2\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "import wandb\n",
    "import wandb.integration.ultralytics\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    train_images_folder: str\n",
    "    train_labels_folder: str\n",
    "    val_images_folder: str\n",
    "    val_labels_folder: str\n",
    "    train_csv: str\n",
    "    val_csv: str\n",
    "    yolo_config_yaml: str\n",
    "    training_output_folder: str\n",
    "    saved_weights_filepath: str\n",
    "    device: str\n",
    "\n",
    "    # noinspection PyAttributeOutsideInit\n",
    "    def init(self, training):\n",
    "        self.training = training\n",
    "        if self.training:\n",
    "            os.makedirs(self.training_output_folder, exist_ok=True)\n",
    "\n",
    "        self.train_ids = pd.read_csv(self.train_csv)['id'].to_numpy()\n",
    "        self.val_ids = pd.read_csv(self.val_csv)['id'].to_numpy()\n",
    "\n",
    "        self.seed = 8675309\n",
    "        self.batch_size = 32\n",
    "        self.starting_learning_rate = 3e-4\n",
    "        self.max_epochs = 40\n",
    "        self.patience = 4\n",
    "        self.num_workers = 8 if self.device == 'cuda' else 0\n",
    "        self.pin_memory = self.num_workers > 0\n",
    "        self.image_size = 640\n",
    "        self.use_amp = self.device == 'cuda'\n",
    "        self.verbose = False\n",
    "\n",
    "        self.imagenet_mean_cpu_tensor = torch.tensor(imagenet_mean_array)\n",
    "        self.imagenet_std_cpu_tensor = torch.tensor(imagenet_std_array)\n",
    "        self.channelwise_imagenet_mean_cpu_tensor = self.imagenet_mean_cpu_tensor.view(3, 1, 1)\n",
    "        self.channelwise_imagenet_std_cpu_tensor = self.imagenet_std_cpu_tensor.view(3, 1, 1)\n",
    "        self.imagenet_mean_gpu_tensor = gpu_tensor(imagenet_mean_array)\n",
    "        self.imagenet_std_gpu_tensor = gpu_tensor(imagenet_std_array)\n",
    "        self.channelwise_imagenet_mean_gpu_tensor = self.imagenet_mean_gpu_tensor.view(3, 1, 1)\n",
    "        self.channelwise_imagenet_std_gpu_tensor = self.imagenet_std_gpu_tensor.view(3, 1, 1)\n",
    "\n",
    "        self.model_name = 'yolo26s.pt'\n",
    "\n",
    "\n",
    "config: Config = None\n",
    "\"\"\" Set to environment-relevant config before training/inference \"\"\";"
   ],
   "id": "14d963bdf3e33354",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "local_config = Config(\n",
    "    train_images_folder='data/license_plates/images/train/',\n",
    "    train_labels_folder='data/license_plates/labels/train/',\n",
    "    val_images_folder='data/license_plates/images/val/',\n",
    "    val_labels_folder='data/license_plates/labels/val/',\n",
    "    train_csv='data/train.csv',\n",
    "    val_csv='data/val.csv',\n",
    "    yolo_config_yaml='data/license_plates.yaml',\n",
    "    training_output_folder='data_gen/',\n",
    "    saved_weights_filepath='data_gen/yolo_best_weights.pt',\n",
    "    device='cpu',\n",
    ")\n",
    "kaggle_config = Config(\n",
    "    train_images_folder='N/A',\n",
    "    train_labels_folder='N/A',\n",
    "    val_images_folder='N/A',\n",
    "    val_labels_folder='N/A',\n",
    "    train_csv='N/A',\n",
    "    val_csv='N/A',\n",
    "    yolo_config_yaml='/kaggle/input/license_plates_dataset/license_plates.yaml',\n",
    "    training_output_folder='/kaggle/working/',\n",
    "    saved_weights_filepath='/kaggle/working/yolo_best_weights.pt',\n",
    "    device='cuda',\n",
    ")"
   ],
   "id": "97f8809277272438",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "imagenet_mean_tuple = (0.485, 0.456, 0.406)\n",
    "imagenet_std_tuple = (0.229, 0.224, 0.225)\n",
    "imagenet_mean_array = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "imagenet_std_array = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "\n",
    "def gpu_tensor(numpy_array):\n",
    "    return torch.tensor(numpy_array, device=config.device)\n",
    "\n",
    "def gpu_image_tensor_to_numpy_array(image_tensor):\n",
    "    image = denormalize(image_tensor, config.channelwise_imagenet_mean_gpu_tensor, config.channelwise_imagenet_std_gpu_tensor)\n",
    "    image = torch.clamp(image, 0, 1)\n",
    "    image = image.permute(1, 2, 0).cpu().numpy()\n",
    "    return (image * 255).astype(np.uint8)\n",
    "\n",
    "def normalize(tensor, mean, std):\n",
    "    return (tensor - mean) / std\n",
    "\n",
    "def denormalize(tensor, mean, std):\n",
    "    return tensor * std + mean\n",
    "\n",
    "def print_model_torchinfo(model: nn.Module):\n",
    "    print(torch_summary(model, input_size=(1, 3, config.image_width, config.image_height)))\n",
    "\n",
    "def print_model(model: nn.Module):\n",
    "    for name, module in model.named_modules():\n",
    "        print(name, \"->\", module.__class__.__name__)\n",
    "\n",
    "def create_dataloader(dataset, shuffle):\n",
    "    return DataLoader(dataset, batch_size=config.batch_size, shuffle=shuffle, num_workers=config.num_workers, pin_memory=config.pin_memory, generator=config.generator)\n",
    "\n",
    "def _num_batches(dataloader):\n",
    "    return math.ceil(len(dataloader.dataset) / config.batch_size)"
   ],
   "id": "200ad60b4749b17b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_images_with_bounding_boxes(images, pred_bounding_boxes=None, true_bounding_boxes=None):\n",
    "    for i, image in enumerate(images):\n",
    "        h, w = image.shape[:2]\n",
    "        fig = px.imshow(image)\n",
    "\n",
    "        if true_bounding_boxes is not None:\n",
    "            for (x_center, y_center, bw, bh) in true_bounding_boxes[i]:\n",
    "                x0 = (x_center - bw / 2) * w\n",
    "                y0 = (y_center - bh / 2) * h\n",
    "                x1 = (x_center + bw / 2) * w\n",
    "                y1 = (y_center + bh / 2) * h\n",
    "                fig.add_shape(type='rect', x0=x0, y0=y0, x1=x1, y1=y1, line_color='orange')\n",
    "\n",
    "        if pred_bounding_boxes is not None:\n",
    "            for (x_center, y_center, bw, bh) in pred_bounding_boxes[i]:\n",
    "                x0 = (x_center - bw / 2) * w\n",
    "                y0 = (y_center - bh / 2) * h\n",
    "                x1 = (x_center + bw / 2) * w\n",
    "                y1 = (y_center + bh / 2) * h\n",
    "                fig.add_shape(type='rect', x0=x0, y0=y0, x1=x1, y1=y1, line_color='blue')\n",
    "\n",
    "        fig.show()"
   ],
   "id": "5fe23ab71708e447"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_val_images_with_ground_truth_bounding_boxes(num_images_to_show=3):\n",
    "    ids = np.random.choice(config.val_ids, num_images_to_show)\n",
    "    images = []\n",
    "    all_boxes = []\n",
    "    for image_id in ids:\n",
    "        image_path = f'{config.val_images_folder}{image_id}.jpg'\n",
    "        image_label = f'{config.val_labels_folder}{image_id}.txt'\n",
    "        image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "        images.append(image)\n",
    "        boxes = []\n",
    "        with open(image_label, 'r') as label_file:\n",
    "            for line in label_file:\n",
    "                coords = [float(c) for c in line.strip().split()[-4:]]\n",
    "                boxes.append(coords)\n",
    "        all_boxes.append(boxes)\n",
    "    plot_images_with_bounding_boxes(images, true_bounding_boxes=all_boxes)"
   ],
   "id": "98ea5b1ed1bc5ea8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# config = local_config\n",
    "# config.init(training=False)\n",
    "# plot_val_images_with_ground_truth_bounding_boxes()"
   ],
   "id": "3fcec4f3152115e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_yolo():\n",
    "    start_time = time.time()\n",
    "    print(f't=0: Initializing training setup...')\n",
    "\n",
    "    # Initialize wandb run (logs hyperparams and enables automatic metric logging)\n",
    "    wandb.init(\n",
    "        project=\"license-plate-detection\",\n",
    "        name=f'run={int(start_time)}',\n",
    "        config={\n",
    "            \"model\": config.model_name,\n",
    "            \"epochs\": config.max_epochs,\n",
    "            \"batch_size\": config.batch_size,\n",
    "            \"image_size\": config.image_size,\n",
    "            \"lr0\": config.starting_learning_rate,\n",
    "            \"patience\": config.patience,\n",
    "            \"seed\": config.seed,\n",
    "            \"amp\": config.use_amp,\n",
    "            \"device\": config.device,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    model = YOLO(config.model_name)\n",
    "\n",
    "    # Register wandb callback — logs metrics per epoch and checkpoints best model as artifact\n",
    "    wandb.integration.ultralytics.add_wandb_callback(model, enable_model_checkpointing=True)\n",
    "\n",
    "    # Train — verbose=True enables per-epoch cell output logging\n",
    "    results = model.train(\n",
    "        data=config.yolo_config_yaml,\n",
    "        epochs=config.max_epochs,\n",
    "        imgsz=config.image_size,\n",
    "        batch=config.batch_size,\n",
    "        lr0=config.starting_learning_rate,\n",
    "        patience=config.patience,\n",
    "        seed=config.seed,\n",
    "        amp=config.use_amp,\n",
    "        workers=config.num_workers,\n",
    "        device=config.device,\n",
    "        verbose=True,\n",
    "        plots=True,\n",
    "    )\n",
    "\n",
    "    # Copy best model weights to config.training_output_folder\n",
    "    best_src = Path(results.save_dir) / 'weights' / 'best.pt'\n",
    "    os.makedirs(config.training_output_folder, exist_ok=True)\n",
    "    best_dst = Path(config.training_output_folder) / 'best.pt'\n",
    "    shutil.copy(str(best_src), str(best_dst))\n",
    "    print(f'Best model saved to: {best_dst}')\n",
    "\n",
    "    wandb.finish()\n",
    "    return results"
   ],
   "id": "e26111490ceb6b70"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def predict_yolo():\n",
    "    model = YOLO(config.saved_weights_filepath)\n",
    "\n",
    "    # Run validation — logs all available COCO metrics to stdout.\n",
    "    # save_json=True triggers pycocotools COCOeval, which prints all 12 standard\n",
    "    # COCO metrics via summarize() when the dataset provides COCO-format annotations.\n",
    "    # verbose=True prints the per-class mAP table from ultralytics.\n",
    "    val_results = model.val(\n",
    "        data=config.yolo_config_yaml,\n",
    "        imgsz=config.image_size,\n",
    "        batch=config.batch_size,\n",
    "        workers=config.num_workers,\n",
    "        device=config.device,\n",
    "        verbose=True,\n",
    "        plots=True,\n",
    "        save_json=True,\n",
    "    )\n",
    "\n",
    "    # Print the three COCO metrics ultralytics always surfaces natively.\n",
    "    # (The remaining 9 area-specific / AR metrics are printed by pycocotools above.)\n",
    "    print(\"\\n--- COCO Metrics (ultralytics-native) ---\")\n",
    "    print(f\"  AP @[IoU=0.50:0.95 | area=all | maxDets=100] = {val_results.box.map:.3f}\")\n",
    "    print(f\"  AP @[IoU=0.50      | area=all | maxDets=100] = {val_results.box.map50:.3f}\")\n",
    "    print(f\"  AP @[IoU=0.75      | area=all | maxDets=100] = {val_results.box.map75:.3f}\")\n",
    "\n",
    "    # Run per-image inference on the val set to collect bounding boxes for visualization.\n",
    "    # model.val() doesn't expose per-image Results objects, so model.predict() is used.\n",
    "    predictions = model.predict(\n",
    "        source=config.val_images_folder,\n",
    "        imgsz=config.image_size,\n",
    "        device=config.device,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    return predictions"
   ],
   "id": "1b69e55026919094"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# wandb_key = user_secrets.get_secret(\"wandb_key\")\n",
    "# !wandb login $wandb_key"
   ],
   "id": "4bb1b04729e7895e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "config = kaggle_config\n",
    "config.init(training=True)\n",
    "train_yolo()"
   ],
   "id": "941c31940a1f6b54"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
