{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install ultralytics",
   "id": "3aee58f9e795687"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import math\n",
    "import shutil\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary as torch_summary\n",
    "import cv2\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "import wandb\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    train_images_folder: str\n",
    "    train_labels_folder: str\n",
    "    val_images_folder: str\n",
    "    val_labels_folder: str\n",
    "    train_csv: str\n",
    "    val_csv: str\n",
    "    yolo_config_yaml: str\n",
    "    training_output_folder: str\n",
    "    saved_weights_filepath: str\n",
    "    video_input_filepath: str\n",
    "    video_output_filepath: str\n",
    "    device: str\n",
    "\n",
    "    # noinspection PyAttributeOutsideInit\n",
    "    def init(self, training):\n",
    "        self.training = training\n",
    "        if self.training:\n",
    "            os.makedirs(self.training_output_folder, exist_ok=True)\n",
    "\n",
    "        # self.train_ids = pd.read_csv(self.train_csv)['id'].to_numpy()\n",
    "        # self.val_ids = pd.read_csv(self.val_csv)['id'].to_numpy()\n",
    "\n",
    "        self.seed = 8675309\n",
    "        self.batch_size = 32\n",
    "        self.starting_learning_rate = 3e-4\n",
    "        self.max_epochs = 40\n",
    "        self.patience = 4\n",
    "        self.num_workers = 8 if self.device == 'cuda' else 0\n",
    "        self.pin_memory = self.num_workers > 0\n",
    "        self.image_size = 640\n",
    "        self.use_amp = self.device == 'cuda'\n",
    "        self.verbose = False\n",
    "\n",
    "        self.imagenet_mean_cpu_tensor = torch.tensor(imagenet_mean_array)\n",
    "        self.imagenet_std_cpu_tensor = torch.tensor(imagenet_std_array)\n",
    "        self.channelwise_imagenet_mean_cpu_tensor = self.imagenet_mean_cpu_tensor.view(3, 1, 1)\n",
    "        self.channelwise_imagenet_std_cpu_tensor = self.imagenet_std_cpu_tensor.view(3, 1, 1)\n",
    "        self.imagenet_mean_gpu_tensor = gpu_tensor(imagenet_mean_array)\n",
    "        self.imagenet_std_gpu_tensor = gpu_tensor(imagenet_std_array)\n",
    "        self.channelwise_imagenet_mean_gpu_tensor = self.imagenet_mean_gpu_tensor.view(3, 1, 1)\n",
    "        self.channelwise_imagenet_std_gpu_tensor = self.imagenet_std_gpu_tensor.view(3, 1, 1)\n",
    "\n",
    "        self.model_name = 'yolo26s.pt'\n",
    "\n",
    "\n",
    "config: Config = None\n",
    "\"\"\" Set to environment-relevant config before training/inference \"\"\";"
   ],
   "id": "14d963bdf3e33354",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "local_config = Config(\n",
    "    train_images_folder='data/license_plates/images/train/',\n",
    "    train_labels_folder='data/license_plates/labels/train/',\n",
    "    val_images_folder='data/license_plates/images/val/',\n",
    "    val_labels_folder='data/license_plates/labels/val/',\n",
    "    train_csv='data/train.csv',\n",
    "    val_csv='data/val.csv',\n",
    "    yolo_config_yaml='data/license_plates.yaml',\n",
    "    training_output_folder='data_gen/',\n",
    "    saved_weights_filepath='data_gen/yolo_best_weights.pt',\n",
    "    video_input_filepath='data/license_plate_video.mp4',\n",
    "    video_output_filepath='data_gen/license_plate_video_inference.mp4',\n",
    "    device='cpu',\n",
    ")\n",
    "kaggle_config = Config(\n",
    "    train_images_folder='N/A',\n",
    "    train_labels_folder='N/A',\n",
    "    val_images_folder='/kaggle/input/datasets/kyledunne/license-plates-dataset/data/license_plates/images/val/',\n",
    "    val_labels_folder='N/A',\n",
    "    train_csv='N/A',\n",
    "    val_csv='N/A',\n",
    "    yolo_config_yaml='/kaggle/input/datasets/kyledunne/license-plates-kaggle-yaml/license_plates_kaggle.yaml',\n",
    "    training_output_folder='/kaggle/working/',\n",
    "    saved_weights_filepath='/kaggle/input/datasets/kyledunne/license-plates-yolo-best-weights-sanity-1/best.pt',\n",
    "    video_input_filepath='/kaggle/input/license_plate_input_video/license_plate_input_video.mp4',\n",
    "    video_output_filepath='/kaggle/working/license_plate_video_inference.mp4',\n",
    "    device='cuda',\n",
    ")"
   ],
   "id": "97f8809277272438",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "imagenet_mean_tuple = (0.485, 0.456, 0.406)\n",
    "imagenet_std_tuple = (0.229, 0.224, 0.225)\n",
    "imagenet_mean_array = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "imagenet_std_array = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "\n",
    "def gpu_tensor(numpy_array):\n",
    "    return torch.tensor(numpy_array, device=config.device)\n",
    "\n",
    "def gpu_image_tensor_to_numpy_array(image_tensor):\n",
    "    image = denormalize(image_tensor, config.channelwise_imagenet_mean_gpu_tensor, config.channelwise_imagenet_std_gpu_tensor)\n",
    "    image = torch.clamp(image, 0, 1)\n",
    "    image = image.permute(1, 2, 0).cpu().numpy()\n",
    "    return (image * 255).astype(np.uint8)\n",
    "\n",
    "def normalize(tensor, mean, std):\n",
    "    return (tensor - mean) / std\n",
    "\n",
    "def denormalize(tensor, mean, std):\n",
    "    return tensor * std + mean\n",
    "\n",
    "def print_model_torchinfo(model: nn.Module):\n",
    "    print(torch_summary(model, input_size=(1, 3, config.image_width, config.image_height)))\n",
    "\n",
    "def print_model(model: nn.Module):\n",
    "    for name, module in model.named_modules():\n",
    "        print(name, \"->\", module.__class__.__name__)\n",
    "\n",
    "def create_dataloader(dataset, shuffle):\n",
    "    return DataLoader(dataset, batch_size=config.batch_size, shuffle=shuffle, num_workers=config.num_workers, pin_memory=config.pin_memory, generator=config.generator)\n",
    "\n",
    "def _num_batches(dataloader):\n",
    "    return math.ceil(len(dataloader.dataset) / config.batch_size)"
   ],
   "id": "200ad60b4749b17b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_images_with_bounding_boxes(images, pred_bounding_boxes=None, pred_boxes_confidence=None, true_bounding_boxes=None):\n",
    "    for i, image in enumerate(images):\n",
    "        h, w = image.shape[:2]\n",
    "        fig = px.imshow(image)\n",
    "\n",
    "        if true_bounding_boxes is not None:\n",
    "            for j, (x_center, y_center, bw, bh) in enumerate(true_bounding_boxes[i]):\n",
    "                x0 = (x_center - bw / 2) * w\n",
    "                y0 = (y_center - bh / 2) * h\n",
    "                x1 = (x_center + bw / 2) * w\n",
    "                y1 = (y_center + bh / 2) * h\n",
    "                fig.add_shape(type='rect', x0=x0, y0=y0, x1=x1, y1=y1, line_color='orange')\n",
    "\n",
    "        if pred_bounding_boxes is not None:\n",
    "            for j, (x_center, y_center, bw, bh) in enumerate(pred_bounding_boxes[i]):\n",
    "                x0 = (x_center - bw / 2) * w\n",
    "                y0 = (y_center - bh / 2) * h\n",
    "                x1 = (x_center + bw / 2) * w\n",
    "                y1 = (y_center + bh / 2) * h\n",
    "                fig.add_shape(type='rect', x0=x0, y0=y0, x1=x1, y1=y1, line_color='blue')\n",
    "                if pred_boxes_confidence is not None:\n",
    "                    label = f\"{int(pred_boxes_confidence[i][j] * 100)}%\"\n",
    "                    fig.add_annotation(x=x0, y=y0, text=label, showarrow=False,\n",
    "                                       font=dict(color='blue'), xanchor='left', yanchor='bottom')\n",
    "\n",
    "        fig.show()"
   ],
   "id": "5fe23ab71708e447"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_val_images_with_ground_truth_bounding_boxes(num_images_to_show=3):\n",
    "    ids = np.random.choice(config.val_ids, num_images_to_show)\n",
    "    images = []\n",
    "    all_boxes = []\n",
    "    for image_id in ids:\n",
    "        image_path = f'{config.val_images_folder}{image_id}.jpg'\n",
    "        image_label = f'{config.val_labels_folder}{image_id}.txt'\n",
    "        image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "        images.append(image)\n",
    "        boxes = []\n",
    "        with open(image_label, 'r') as label_file:\n",
    "            for line in label_file:\n",
    "                coords = [float(c) for c in line.strip().split()[-4:]]\n",
    "                boxes.append(coords)\n",
    "        all_boxes.append(boxes)\n",
    "    plot_images_with_bounding_boxes(images, true_bounding_boxes=all_boxes)"
   ],
   "id": "98ea5b1ed1bc5ea8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_yolo():\n",
    "    start_time = time.time()\n",
    "    print(f't=0: Initializing training setup...')\n",
    "\n",
    "    # Initialize wandb run (logs hyperparams and enables automatic metric logging)\n",
    "    wandb.init(\n",
    "        project=\"license-plate-detection\",\n",
    "        name=f'run={int(start_time)}',\n",
    "        config={\n",
    "            \"model\": config.model_name,\n",
    "            \"epochs\": config.max_epochs,\n",
    "            \"batch_size\": config.batch_size,\n",
    "            \"image_size\": config.image_size,\n",
    "            \"lr0\": config.starting_learning_rate,\n",
    "            \"patience\": config.patience,\n",
    "            \"seed\": config.seed,\n",
    "            \"amp\": config.use_amp,\n",
    "            \"device\": config.device,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    model = YOLO(config.model_name)\n",
    "\n",
    "    # Train — verbose=True enables per-epoch cell output logging\n",
    "    results = model.train(\n",
    "        data=config.yolo_config_yaml,\n",
    "        epochs=config.max_epochs,\n",
    "        imgsz=config.image_size,\n",
    "        batch=config.batch_size,\n",
    "        lr0=config.starting_learning_rate,\n",
    "        patience=config.patience,\n",
    "        seed=config.seed,\n",
    "        amp=config.use_amp,\n",
    "        workers=config.num_workers,\n",
    "        device=config.device,\n",
    "        verbose=True,\n",
    "        plots=True,\n",
    "    )\n",
    "\n",
    "    # Copy best model weights to config.training_output_folder\n",
    "    best_src = Path(results.save_dir) / 'weights' / 'best.pt'\n",
    "    os.makedirs(config.training_output_folder, exist_ok=True)\n",
    "    best_dst = Path(config.training_output_folder) / 'best.pt'\n",
    "    shutil.copy(str(best_src), str(best_dst))\n",
    "    print(f'Best model saved to: {best_dst}')\n",
    "\n",
    "    # Upload best model to wandb as an artifact (replaces add_wandb_callback checkpointing,\n",
    "    # which is broken in kaggle environment due to unresolved imports in the callback module)\n",
    "    artifact = wandb.Artifact(name='best-model', type='model')\n",
    "    artifact.add_file(str(best_dst))\n",
    "    wandb.log_artifact(artifact)\n",
    "\n",
    "    wandb.finish()\n",
    "    return results\n"
   ],
   "id": "e26111490ceb6b70"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def predict_yolo():\n",
    "    model = YOLO(config.saved_weights_filepath)\n",
    "\n",
    "    # Run validation — logs all available COCO metrics to stdout.\n",
    "    # save_json=True triggers pycocotools COCOeval, which prints all 12 standard\n",
    "    # COCO metrics via summarize() when the dataset provides COCO-format annotations.\n",
    "    # verbose=True prints the per-class mAP table from ultralytics.\n",
    "    val_results = model.val(\n",
    "        data=config.yolo_config_yaml,\n",
    "        imgsz=config.image_size,\n",
    "        batch=config.batch_size,\n",
    "        workers=config.num_workers,\n",
    "        device=config.device,\n",
    "        verbose=True,\n",
    "        plots=True,\n",
    "        save_json=True,\n",
    "    )\n",
    "\n",
    "    # Print the three COCO metrics ultralytics always surfaces natively.\n",
    "    # (The remaining 9 area-specific / AR metrics are printed by pycocotools above.)\n",
    "    print('\\n--- COCO Metrics (ultralytics-native) ---')\n",
    "    print(f'  AP @[IoU=0.50:0.95 | area=all | maxDets=100] = {val_results.box.map:.3f}')\n",
    "    print(f'  AP @[IoU=0.50      | area=all | maxDets=100] = {val_results.box.map50:.3f}')\n",
    "    print(f'  AP @[IoU=0.75      | area=all | maxDets=100] = {val_results.box.map75:.3f}')\n",
    "\n",
    "    # Run per-image inference on the val set to collect bounding boxes for visualization.\n",
    "    # model.val() doesn't expose per-image Results objects, so model.predict() is used.\n",
    "    predictions = model.predict(\n",
    "        source=config.val_images_folder,\n",
    "        imgsz=config.image_size,\n",
    "        device=config.device,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    return predictions"
   ],
   "id": "1b69e55026919094"
  },
  {
   "cell_type": "code",
   "id": "uqwfhvan72m",
   "source": [
    "def predict_yolo_with_visualizations(num_to_visualize=3):\n",
    "    predictions = predict_yolo()\n",
    "\n",
    "    indices = np.random.choice(len(predictions), num_to_visualize, replace=False)\n",
    "    subset = [predictions[i] for i in indices]\n",
    "\n",
    "    images = [cv2.cvtColor(r.orig_img, cv2.COLOR_BGR2RGB) for r in subset]\n",
    "    pred_bounding_boxes = [r.boxes.xywhn.cpu().numpy().tolist() for r in subset]\n",
    "    pred_boxes_confidence = [r.boxes.conf.cpu().numpy().tolist() for r in subset]\n",
    "\n",
    "    plot_images_with_bounding_boxes(\n",
    "        images,\n",
    "        pred_bounding_boxes=pred_bounding_boxes,\n",
    "        pred_boxes_confidence=pred_boxes_confidence,\n",
    "    )"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def predict_video():\n",
    "    model = YOLO(config.saved_weights_filepath)\n",
    "\n",
    "    video = cv2.VideoCapture(config.video_input_filepath)\n",
    "    if not video.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "\n",
    "    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    out_dir = os.path.dirname(config.video_output_filepath)\n",
    "    if out_dir:\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "    output_file = cv2.VideoWriter(\n",
    "        filename=config.video_output_filepath,\n",
    "        fourcc=cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "        fps=float(fps),\n",
    "        frameSize=(width, height),\n",
    "        isColor=True,\n",
    "    )\n",
    "\n",
    "    while video.isOpened():\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        results = model.predict(\n",
    "            source=frame,\n",
    "            imgsz=config.image_size,\n",
    "            device=config.device,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        for result in results:\n",
    "            boxes = result.boxes.xyxy.cpu().numpy()  # [N, 4] pixel coords (x1, y1, x2, y2)\n",
    "            confs = result.boxes.conf.cpu().numpy()  # [N]\n",
    "            for (x1, y1, x2, y2), conf in zip(boxes, confs):\n",
    "                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color=(0, 165, 255), thickness=2)\n",
    "                label = f'{int(conf * 100)}%'\n",
    "                cv2.putText(frame, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            fontScale=0.6, color=(0, 165, 255), thickness=2)\n",
    "\n",
    "        output_file.write(frame)\n",
    "\n",
    "    video.release()\n",
    "    output_file.release()\n",
    "    print(f'Video saved to: {config.video_output_filepath}')"
   ],
   "id": "d3c53164c78159df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "wandb_key = user_secrets.get_secret(\"wandb_key\")\n",
    "!wandb login $wandb_key"
   ],
   "id": "4bb1b04729e7895e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "config = kaggle_config\n",
    "config.init(training=True)\n",
    "train_yolo()"
   ],
   "id": "941c31940a1f6b54"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
