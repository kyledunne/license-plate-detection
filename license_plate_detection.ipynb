{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# !pip install ultralytics",
   "id": "3aee58f9e795687",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import math\n",
    "import shutil\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary as torch_summary\n",
    "import cv2\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "import wandb\n",
    "import time\n",
    "import json\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    train_images_folder: str\n",
    "    train_labels_folder: str\n",
    "    val_images_folder: str\n",
    "    val_labels_folder: str\n",
    "    train_csv: str\n",
    "    val_csv: str\n",
    "    yolo_config_yaml: str\n",
    "    training_output_folder: str\n",
    "    saved_weights_filepath: str\n",
    "    video_input_filepath: str\n",
    "    video_output_filepath: str\n",
    "    device: str\n",
    "\n",
    "    # noinspection PyAttributeOutsideInit\n",
    "    def init(self, training):\n",
    "        self.training = training\n",
    "        if self.training:\n",
    "            os.makedirs(self.training_output_folder, exist_ok=True)\n",
    "\n",
    "        self.seed = 8675309\n",
    "        self.batch_size = 32\n",
    "        self.starting_learning_rate = 3e-4\n",
    "        self.max_epochs = 40\n",
    "        self.patience = 4\n",
    "        self.num_workers = 8 if self.device == 'cuda' else 0\n",
    "        self.pin_memory = self.num_workers > 0\n",
    "        self.image_size = 640\n",
    "        self.use_amp = self.device == 'cuda'\n",
    "        self.verbose = False\n",
    "\n",
    "        if self.val_csv != 'N/A':\n",
    "            self.val_ids = pd.read_csv(self.val_csv)['id'].to_numpy()\n",
    "\n",
    "        self.imagenet_mean_cpu_tensor = torch.tensor(imagenet_mean_array)\n",
    "        self.imagenet_std_cpu_tensor = torch.tensor(imagenet_std_array)\n",
    "        self.channelwise_imagenet_mean_cpu_tensor = self.imagenet_mean_cpu_tensor.view(3, 1, 1)\n",
    "        self.channelwise_imagenet_std_cpu_tensor = self.imagenet_std_cpu_tensor.view(3, 1, 1)\n",
    "        self.imagenet_mean_gpu_tensor = gpu_tensor(imagenet_mean_array)\n",
    "        self.imagenet_std_gpu_tensor = gpu_tensor(imagenet_std_array)\n",
    "        self.channelwise_imagenet_mean_gpu_tensor = self.imagenet_mean_gpu_tensor.view(3, 1, 1)\n",
    "        self.channelwise_imagenet_std_gpu_tensor = self.imagenet_std_gpu_tensor.view(3, 1, 1)\n",
    "\n",
    "        self.model_name = 'yolo26s.pt'\n",
    "\n",
    "\n",
    "config: Config = None\n",
    "\"\"\" Set to environment-relevant config before training/inference \"\"\";"
   ],
   "id": "14d963bdf3e33354",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "local_config = Config(\n",
    "    train_images_folder='data/license_plates/images/train/',\n",
    "    train_labels_folder='data/license_plates/labels/train/',\n",
    "    val_images_folder='data/license_plates/images/val/',\n",
    "    val_labels_folder='data/license_plates/labels/val/',\n",
    "    train_csv='data/train.csv',\n",
    "    val_csv='data/val.csv',\n",
    "    yolo_config_yaml='data/license_plates.yaml',\n",
    "    training_output_folder='data_gen/',\n",
    "    saved_weights_filepath='data_gen/best.pt',\n",
    "    video_input_filepath='data/license_plate_video.mp4',\n",
    "    video_output_filepath='data_gen/license_plate_video_inference.mp4',\n",
    "    device='cpu',\n",
    ")\n",
    "kaggle_config = Config(\n",
    "    train_images_folder='N/A',\n",
    "    train_labels_folder='N/A',\n",
    "    val_images_folder='/kaggle/input/datasets/kyledunne/license-plates-dataset/data/license_plates/images/val/',\n",
    "    val_labels_folder='/kaggle/input/datasets/kyledunne/license-plates-dataset/data/license_plates/labels/val/',\n",
    "    train_csv='N/A',\n",
    "    val_csv='N/A',\n",
    "    yolo_config_yaml='/kaggle/input/datasets/kyledunne/license-plates-kaggle-yaml/license_plates_kaggle.yaml',\n",
    "    training_output_folder='/kaggle/working/',\n",
    "    saved_weights_filepath='/kaggle/input/datasets/kyledunne/license-plates-yolo-best-weights-sanity-1/best.pt',\n",
    "    video_input_filepath='/kaggle/input/license_plate_input_video/license_plate_input_video.mp4',\n",
    "    video_output_filepath='/kaggle/working/license_plate_video_inference.mp4',\n",
    "    device='cuda',\n",
    ")"
   ],
   "id": "97f8809277272438",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "imagenet_mean_tuple = (0.485, 0.456, 0.406)\n",
    "imagenet_std_tuple = (0.229, 0.224, 0.225)\n",
    "imagenet_mean_array = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "imagenet_std_array = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "\n",
    "def gpu_tensor(numpy_array):\n",
    "    return torch.tensor(numpy_array, device=config.device)\n",
    "\n",
    "def gpu_image_tensor_to_numpy_array(image_tensor):\n",
    "    image = denormalize(image_tensor, config.channelwise_imagenet_mean_gpu_tensor, config.channelwise_imagenet_std_gpu_tensor)\n",
    "    image = torch.clamp(image, 0, 1)\n",
    "    image = image.permute(1, 2, 0).cpu().numpy()\n",
    "    return (image * 255).astype(np.uint8)\n",
    "\n",
    "def normalize(tensor, mean, std):\n",
    "    return (tensor - mean) / std\n",
    "\n",
    "def denormalize(tensor, mean, std):\n",
    "    return tensor * std + mean\n",
    "\n",
    "def print_model_torchinfo(model: nn.Module):\n",
    "    print(torch_summary(model, input_size=(1, 3, config.image_width, config.image_height)))\n",
    "\n",
    "def print_model(model: nn.Module):\n",
    "    for name, module in model.named_modules():\n",
    "        print(name, \"->\", module.__class__.__name__)\n",
    "\n",
    "def create_dataloader(dataset, shuffle):\n",
    "    return DataLoader(dataset, batch_size=config.batch_size, shuffle=shuffle, num_workers=config.num_workers, pin_memory=config.pin_memory, generator=config.generator)\n",
    "\n",
    "def _num_batches(dataloader):\n",
    "    return math.ceil(len(dataloader.dataset) / config.batch_size)"
   ],
   "id": "200ad60b4749b17b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_images_with_bounding_boxes(images, pred_bounding_boxes=None, pred_boxes_confidence=None, true_bounding_boxes=None):\n",
    "    for i, image in enumerate(images):\n",
    "        h, w = image.shape[:2]\n",
    "        fig = px.imshow(image)\n",
    "\n",
    "        if true_bounding_boxes is not None:\n",
    "            for j, (x_center, y_center, bw, bh) in enumerate(true_bounding_boxes[i]):\n",
    "                x0 = (x_center - bw / 2) * w\n",
    "                y0 = (y_center - bh / 2) * h\n",
    "                x1 = (x_center + bw / 2) * w\n",
    "                y1 = (y_center + bh / 2) * h\n",
    "                fig.add_shape(type='rect', x0=x0, y0=y0, x1=x1, y1=y1, line_color='orange')\n",
    "\n",
    "        if pred_bounding_boxes is not None:\n",
    "            for j, (x_center, y_center, bw, bh) in enumerate(pred_bounding_boxes[i]):\n",
    "                x0 = (x_center - bw / 2) * w\n",
    "                y0 = (y_center - bh / 2) * h\n",
    "                x1 = (x_center + bw / 2) * w\n",
    "                y1 = (y_center + bh / 2) * h\n",
    "                fig.add_shape(type='rect', x0=x0, y0=y0, x1=x1, y1=y1, line_color='blue')\n",
    "                if pred_boxes_confidence is not None:\n",
    "                    label = f\"{int(pred_boxes_confidence[i][j] * 100)}%\"\n",
    "                    fig.add_annotation(x=x0, y=y0, text=label, showarrow=False,\n",
    "                                       font=dict(color='blue'), xanchor='left', yanchor='bottom',\n",
    "                                       bgcolor='white', borderpad=2)\n",
    "\n",
    "        fig.show()"
   ],
   "id": "5fe23ab71708e447",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_val_images_with_ground_truth_bounding_boxes(num_images_to_show=3):\n",
    "    ids = np.random.choice(config.val_ids, num_images_to_show)\n",
    "    images = []\n",
    "    all_boxes = []\n",
    "    for image_id in ids:\n",
    "        image_path = f'{config.val_images_folder}{image_id}.jpg'\n",
    "        image_label = f'{config.val_labels_folder}{image_id}.txt'\n",
    "        image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "        images.append(image)\n",
    "        boxes = []\n",
    "        with open(image_label, 'r') as label_file:\n",
    "            for line in label_file:\n",
    "                coords = [float(c) for c in line.strip().split()[-4:]]\n",
    "                boxes.append(coords)\n",
    "        all_boxes.append(boxes)\n",
    "    plot_images_with_bounding_boxes(images, true_bounding_boxes=all_boxes)"
   ],
   "id": "98ea5b1ed1bc5ea8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_yolo():\n",
    "    start_time = time.time()\n",
    "    print(f't=0: Initializing training setup...')\n",
    "\n",
    "    # Initialize wandb run (logs hyperparams and enables automatic metric logging)\n",
    "    wandb.init(\n",
    "        project=\"license-plate-detection\",\n",
    "        name=f'run={int(start_time)}',\n",
    "        config={\n",
    "            \"model\": config.model_name,\n",
    "            \"epochs\": config.max_epochs,\n",
    "            \"batch_size\": config.batch_size,\n",
    "            \"image_size\": config.image_size,\n",
    "            \"lr0\": config.starting_learning_rate,\n",
    "            \"patience\": config.patience,\n",
    "            \"seed\": config.seed,\n",
    "            \"amp\": config.use_amp,\n",
    "            \"device\": config.device,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    model = YOLO(config.model_name)\n",
    "\n",
    "    # Train — verbose=True enables per-epoch cell output logging\n",
    "    results = model.train(\n",
    "        data=config.yolo_config_yaml,\n",
    "        epochs=config.max_epochs,\n",
    "        imgsz=config.image_size,\n",
    "        batch=config.batch_size,\n",
    "        optimizer='AdamW',\n",
    "        lr0=config.starting_learning_rate,\n",
    "        warmup_bias_lr=0.01,\n",
    "        cos_lr=True,\n",
    "        patience=config.patience,\n",
    "        seed=config.seed,\n",
    "        amp=config.use_amp,\n",
    "        workers=config.num_workers,\n",
    "        device=config.device,\n",
    "        verbose=True,\n",
    "        plots=True,\n",
    "    )\n",
    "\n",
    "    # Copy best model weights to config.training_output_folder\n",
    "    best_src = Path(results.save_dir) / 'weights' / 'best.pt'\n",
    "    os.makedirs(config.training_output_folder, exist_ok=True)\n",
    "    best_dst = Path(config.training_output_folder) / 'best.pt'\n",
    "    shutil.copy(str(best_src), str(best_dst))\n",
    "    print(f'Best model saved to: {best_dst}')\n",
    "\n",
    "    # Upload best model to wandb as an artifact (replaces add_wandb_callback checkpointing,\n",
    "    # which is broken in kaggle environment due to unresolved imports in the callback module)\n",
    "    artifact = wandb.Artifact(name='best-model', type='model')\n",
    "    artifact.add_file(str(best_dst))\n",
    "    wandb.log_artifact(artifact)\n",
    "\n",
    "    wandb.finish()\n",
    "    return results"
   ],
   "id": "e26111490ceb6b70",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "joo2t57jrrh",
   "source": [
    "def print_all_coco_metrics(val_results):\n",
    "    \"\"\"\n",
    "    Run pycocotools COCOeval and print all 12 standard COCO metrics.\n",
    "\n",
    "    Ultralytics only triggers pycocotools automatically for officially-structured\n",
    "    COCO datasets (is_coco=True). This dataset uses hex-string filenames, so even\n",
    "    with the COCO layout, image IDs in predictions.json would be strings while\n",
    "    pycocotools requires integers — COCOeval would score zeros. Instead, we build\n",
    "    the COCO ground-truth in memory and remap IDs consistently for both GT and\n",
    "    predictions before running COCOeval.\n",
    "\n",
    "    Requires model.val() to have been called with save_json=True so that\n",
    "    predictions.json exists in val_results.save_dir.\n",
    "    \"\"\"\n",
    "\n",
    "    pred_json_path = Path(val_results.save_dir) / 'predictions.json'\n",
    "    images_dir = Path(config.val_images_folder)\n",
    "    labels_dir = Path(config.val_labels_folder)\n",
    "\n",
    "    # Load predictions (image_id is a hex string, e.g. \"b01d46f9911d558b\")\n",
    "    with open(pred_json_path, 'r') as f:\n",
    "        raw_preds = json.load(f)\n",
    "\n",
    "    if not raw_preds:\n",
    "        print('\\n--- COCO Metrics (pycocotools) ---')\n",
    "        print('  No predictions found — skipping COCOeval.')\n",
    "        return\n",
    "\n",
    "    # pycocotools requires integer IDs. Build a stable str→int mapping from\n",
    "    # all val images (not just predicted ones, so recall denominator is correct).\n",
    "    all_stems = sorted(p.stem for p in images_dir.glob('*.jpg'))\n",
    "    stem_to_id = {stem: i + 1 for i, stem in enumerate(all_stems)}\n",
    "\n",
    "    # Remap predictions to integer IDs\n",
    "    remapped_preds = [\n",
    "        {**pred, 'image_id': stem_to_id[pred['image_id']]}\n",
    "        for pred in raw_preds\n",
    "        if pred['image_id'] in stem_to_id\n",
    "    ]\n",
    "\n",
    "    # Build COCO ground-truth structure in memory\n",
    "    coco_images = []\n",
    "    coco_annotations = []\n",
    "    ann_id = 1\n",
    "\n",
    "    for stem in all_stems:\n",
    "        img = cv2.imread(str(images_dir / f'{stem}.jpg'))\n",
    "        h, w = img.shape[:2] if img is not None else (640, 640)\n",
    "        int_id = stem_to_id[stem]\n",
    "\n",
    "        coco_images.append({'id': int_id, 'file_name': f'{stem}.jpg', 'width': w, 'height': h})\n",
    "\n",
    "        label_path = labels_dir / f'{stem}.txt'\n",
    "        if not label_path.exists() or label_path.stat().st_size == 0:\n",
    "            continue\n",
    "\n",
    "        with open(label_path) as lf:\n",
    "            for line in lf:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) < 5:\n",
    "                    continue\n",
    "                _, xc, yc, bw, bh = map(float, parts)\n",
    "                x = (xc - bw / 2) * w\n",
    "                y = (yc - bh / 2) * h\n",
    "                bw_px, bh_px = bw * w, bh * h\n",
    "                coco_annotations.append({\n",
    "                    'id': ann_id, 'image_id': int_id, 'category_id': 1,\n",
    "                    'bbox': [x, y, bw_px, bh_px], 'area': bw_px * bh_px, 'iscrowd': 0,\n",
    "                })\n",
    "                ann_id += 1\n",
    "\n",
    "    # Load into pycocotools without writing any files\n",
    "    coco_gt = COCO()\n",
    "    coco_gt.dataset = {\n",
    "        'images': coco_images,\n",
    "        'annotations': coco_annotations,\n",
    "        'categories': [{'id': 1, 'name': 'license_plate', 'supercategory': 'vehicle'}],\n",
    "    }\n",
    "    coco_gt.createIndex()\n",
    "    coco_dt = coco_gt.loadRes(remapped_preds)\n",
    "\n",
    "    # Run evaluation and print all 12 standard COCO metrics\n",
    "    print('\\n--- COCO Metrics (pycocotools) ---')\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, iouType='bbox')\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def predict_yolo(return_preds=True):\n",
    "    model = YOLO(config.saved_weights_filepath)\n",
    "\n",
    "    # Run validation — logs all available COCO metrics to stdout.\n",
    "    # save_json=True saves predictions.json to val_results.save_dir, which is read\n",
    "    # by print_all_coco_metrics() to compute all 12 standard COCO metrics.\n",
    "    # verbose=True prints the per-class mAP table from ultralytics.\n",
    "    val_results = model.val(\n",
    "        data=config.yolo_config_yaml,\n",
    "        imgsz=config.image_size,\n",
    "        batch=config.batch_size,\n",
    "        workers=config.num_workers,\n",
    "        device=config.device,\n",
    "        verbose=True,\n",
    "        plots=True,\n",
    "        save_json=True,\n",
    "    )\n",
    "\n",
    "    print_all_coco_metrics(val_results)\n",
    "\n",
    "    if not return_preds:\n",
    "        return\n",
    "\n",
    "    # Run per-image inference on the val set to collect bounding boxes for visualization.\n",
    "    # model.val() doesn't expose per-image Results objects, so model.predict() is used.\n",
    "    predictions = model.predict(\n",
    "        source=config.val_images_folder,\n",
    "        imgsz=config.image_size,\n",
    "        device=config.device,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    return predictions"
   ],
   "id": "1b69e55026919094",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "uqwfhvan72m",
   "source": [
    "def predict_yolo_with_visualizations(num_to_visualize=3):\n",
    "    predictions = predict_yolo()\n",
    "\n",
    "    indices = np.random.choice(len(predictions), num_to_visualize, replace=False)\n",
    "    subset = [predictions[i] for i in indices]\n",
    "\n",
    "    images = [cv2.cvtColor(r.orig_img, cv2.COLOR_BGR2RGB) for r in subset]\n",
    "    pred_bounding_boxes = [r.boxes.xywhn.cpu().numpy().tolist() for r in subset]\n",
    "    pred_boxes_confidence = [r.boxes.conf.cpu().numpy().tolist() for r in subset]\n",
    "\n",
    "    plot_images_with_bounding_boxes(\n",
    "        images,\n",
    "        pred_bounding_boxes=pred_bounding_boxes,\n",
    "        pred_boxes_confidence=pred_boxes_confidence,\n",
    "    )"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def predict_video():\n",
    "    model = YOLO(config.saved_weights_filepath)\n",
    "\n",
    "    video = cv2.VideoCapture(config.video_input_filepath)\n",
    "    if not video.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "\n",
    "    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    out_dir = os.path.dirname(config.video_output_filepath)\n",
    "    if out_dir:\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "    output_file = cv2.VideoWriter(\n",
    "        filename=config.video_output_filepath,\n",
    "        fourcc=cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "        fps=float(fps),\n",
    "        frameSize=(width, height),\n",
    "        isColor=True,\n",
    "    )\n",
    "\n",
    "    while video.isOpened():\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        results = model.predict(\n",
    "            source=frame,\n",
    "            imgsz=config.image_size,\n",
    "            device=config.device,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        for result in results:\n",
    "            boxes = result.boxes.xyxy.cpu().numpy()  # [N, 4] pixel coords (x1, y1, x2, y2)\n",
    "            confs = result.boxes.conf.cpu().numpy()  # [N]\n",
    "            for (x1, y1, x2, y2), conf in zip(boxes, confs):\n",
    "                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color=(0, 165, 255), thickness=2)\n",
    "                label = f'{int(conf * 100)}%'\n",
    "                cv2.putText(frame, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            fontScale=0.6, color=(0, 165, 255), thickness=2)\n",
    "\n",
    "        output_file.write(frame)\n",
    "\n",
    "    video.release()\n",
    "    output_file.release()\n",
    "    print(f'Video saved to: {config.video_output_filepath}')"
   ],
   "id": "d3c53164c78159df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# wandb_key = user_secrets.get_secret(\"wandb_key\")\n",
    "# !wandb login $wandb_key"
   ],
   "id": "4bb1b04729e7895e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config = local_config\n",
    "config.init(training=False)\n",
    "predict_yolo(return_preds=False)"
   ],
   "id": "941c31940a1f6b54",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
